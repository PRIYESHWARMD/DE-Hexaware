{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aaabe6d3-8722-4998-b366-18e14becc79c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Replace with your Event Hub connection string\n",
    "connectionString = \"EVENT_KEY\"\n",
    "\n",
    "ehConf = {\n",
    "  'EVENT_HUB_KEY' : connectionString\n",
    "}\n",
    "\n",
    "# Read stream from Event Hub\n",
    "df = spark.readStream \\\n",
    "    .format(\"eventhubs\") \\\n",
    "    .options(**ehConf) \\\n",
    "    .load()\n",
    "\n",
    "df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "57e6ac6b-41f5-438f-a750-cc54e84c2eb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install azure-eventhub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8eaec050-5836-44e4-a091-f06e8b76bb36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56546f03-81ae-428a-b097-0e2e41823c25",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.eventhub import EventHubConsumerClient\n",
    "\n",
    "CONNECTION_STR = \"EVENT_KEY\"\n",
    "CONSUMER_GROUP = \"traffic-app\"  # or $Default consumer group\n",
    "EVENTHUB_NAME = \"traffic-iot\"\n",
    "\n",
    "def on_event(partition_context, event):\n",
    "    print(f\"Received event from partition: {partition_context.partition_id}\")\n",
    "    print(event.body_as_str())\n",
    "    partition_context.update_checkpoint(event)\n",
    "\n",
    "client = EventHubConsumerClient.from_connection_string(\n",
    "    conn_str=CONNECTION_STR,\n",
    "    consumer_group=CONSUMER_GROUP,\n",
    "    eventhub_name=EVENTHUB_NAME\n",
    ")\n",
    "\n",
    "with client:\n",
    "    client.receive(\n",
    "        on_event=on_event,\n",
    "        starting_position=\"-1\",  # \"-1\" = read from beginning of stream\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "744dc38c-13c0-4fcb-afe5-f6ec52ca9d1c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType\n",
    "from pyspark.sql.functions import col, from_json, coalesce, to_timestamp\n",
    "\n",
    "# ---- 1) Schema: keep everything as STRING, cast later (avoids nulls on type mismatch)\n",
    "schema = (StructType()\n",
    "          .add(\"vehicle_id\", StringType())\n",
    "          .add(\"speed\", StringType())\n",
    "          .add(\"timestamp\", StringType())     # some producers use \"timestamp\"\n",
    "          .add(\"event_time\", StringType()))   # others use \"event_time\"\n",
    "\n",
    "# ---- 2) Secure your connection string (example uses a placeholder)\n",
    "connectionString = \"EVENT_KEY\"\n",
    "eh_conf = {\n",
    "    \"eventhubs.connectionString\": sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connectionString),\n",
    "    \"eventhubs.consumerGroup\": \"$Default\",\n",
    "    # Start from the earliest retained events\n",
    "    \"eventhubs.startingPosition\": \"\"\"{\n",
    "      \"offset\": \"-1\",\n",
    "      \"seqNo\": -1,\n",
    "      \"enqueuedTime\": null,\n",
    "      \"isInclusive\": true\n",
    "    }\"\"\"\n",
    "}\n",
    "\n",
    "# ---- 3) Read Event Hubs stream\n",
    "raw_stream = (spark.readStream\n",
    "              .format(\"eventhubs\")\n",
    "              .options(**eh_conf)\n",
    "              .load())\n",
    "\n",
    "# ---- 4) Parse JSON safely\n",
    "parsed = raw_stream.select(\n",
    "    col(\"enqueuedTime\").alias(\"ingest_time\"),\n",
    "    from_json(col(\"body\").cast(\"string\"), schema).alias(\"data\")\n",
    ")\n",
    "\n",
    "iot_df = (parsed\n",
    "    .select(\n",
    "        col(\"data.vehicle_id\").cast(\"int\").alias(\"vehicle_id\"),\n",
    "        col(\"data.speed\").cast(\"int\").alias(\"speed\"),\n",
    "        # prefer payload time; fall back to Event Hubs enqueued time\n",
    "        coalesce(col(\"data.timestamp\"), col(\"data.event_time\")).alias(\"ts_raw\"),\n",
    "        col(\"ingest_time\")\n",
    "    )\n",
    "    .withColumn(\n",
    "        \"event_time\",\n",
    "        coalesce(\n",
    "            to_timestamp(col(\"ts_raw\")),                     # try default parsing\n",
    "            col(\"ingest_time\").cast(\"timestamp\")             # fallback\n",
    "        )\n",
    "    )\n",
    "    .drop(\"ts_raw\")\n",
    ")\n",
    "\n",
    "# ---- 5) Write to Delta (BRONZE) with a dedicated checkpoint\n",
    "delta_query = (iot_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", \"/mnt/bronze/_checkpoint_iot\")\n",
    "    .option(\"mergeSchema\", \"true\")    # allow new/changed columns\n",
    "    .start(\"/mnt/bronze/iotdata\"))\n",
    "\n",
    "# ---- 6) OPTIONAL: Also stream to console so you can verify rows immediately\n",
    "debug_query = (iot_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .option(\"truncate\", False)\n",
    "    .option(\"numRows\", 20)\n",
    "    .start())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b177de8f-f51e-43b9-a7cc-a710e9708901",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import from_json, col, to_timestamp, current_timestamp\n",
    "\n",
    "# 1) Schema\n",
    "schema = StructType([\n",
    "    StructField(\"vehicle_id\", IntegerType(), True),\n",
    "    StructField(\"speed\", IntegerType(), True),\n",
    "    StructField(\"timestamp\", StringType(), True)  # raw string\n",
    "])\n",
    "\n",
    "# 2) Parse + Add event_time\n",
    "parsed_df = (raw_stream\n",
    "    .select(from_json(col(\"body\").cast(\"string\"), schema).alias(\"data\"))\n",
    "    .select(\"data.*\")\n",
    "    .withColumn(\"event_time\", to_timestamp(\"timestamp\"))  # convert to timestamp\n",
    "    .withColumn(\"ingest_time\", current_timestamp())       # optional lineage\n",
    ")\n",
    "\n",
    "# 3) Paths\n",
    "silver_path = \"dbfs:/mnt/silver/iotdata\"\n",
    "checkpoint_path = \"dbfs:/mnt/silver/_checkpoint_iot\"\n",
    "\n",
    "# 4) Write to Silver (Delta Lake)\n",
    "delta_query = (parsed_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .option(\"mergeSchema\", \"true\")   # allow schema evolution\n",
    "    .start(silver_path)\n",
    ")\n",
    "\n",
    "# 5) Debug Console Sink (for monitoring)\n",
    "console_query = (parsed_df.writeStream\n",
    "    .format(\"console\")\n",
    "    .outputMode(\"append\")\n",
    "    .option(\"truncate\", False)\n",
    "    .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b0f6d6f-b27d-4fbb-883e-0b59b097e7f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# 6) ---- In a separate cell, AFTER stream runs ----\n",
    "# Read from Silver Delta table\n",
    "silver_df = spark.read.format(\"delta\").load(silver_path)\n",
    "\n",
    "# Show only 10 rows\n",
    "silver_df.show(10, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5018ea5e-d8d9-4ff3-b019-0a91b723cb62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, avg, count, window\n",
    "\n",
    "# 1. Read from silver (streaming)\n",
    "silver_df = (\n",
    "    spark.readStream\n",
    "    .format(\"delta\")\n",
    "    .load(\"/mnt/silver/iotdata\")\n",
    ")\n",
    "\n",
    "# 2. Aggregate for gold\n",
    "gold_df = (\n",
    "    silver_df\n",
    "    .withWatermark(\"event_time\", \"10 minutes\")   # use watermark on event_time\n",
    "    .groupBy(\n",
    "        window(col(\"event_time\"), \"5 minutes\"),  # tumbling window of 5 mins\n",
    "        col(\"vehicle_id\")\n",
    "    )\n",
    "    .agg(\n",
    "        avg(\"speed\").alias(\"avg_speed\"),\n",
    "        count(\"*\").alias(\"event_count\")\n",
    "    )\n",
    "    .select(\n",
    "        col(\"window.start\").alias(\"window_start\"),\n",
    "        col(\"window.end\").alias(\"window_end\"),\n",
    "        col(\"vehicle_id\"),\n",
    "        col(\"avg_speed\"),\n",
    "        col(\"event_count\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# 3. Write to gold (Delta table)\n",
    "query = (\n",
    "    gold_df.writeStream\n",
    "    .format(\"delta\")\n",
    "    .outputMode(\"append\")     # required for aggregates with watermark\n",
    "    .option(\"checkpointLocation\", \"/mnt/gold/_checkpoint_iot\")\n",
    "    .start(\"/mnt/gold/iotdata\")\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee0f80eb-36ec-44a1-8a33-db608cacf5ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Register the gold delta folder as a temporary view\n",
    "gold_df_read = spark.read.format(\"delta\").load(\"/mnt/gold/iotdata\")\n",
    "\n",
    "# Show 10 rows\n",
    "gold_df_read.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b66010a0-f985-4e6c-adf5-8c3321bc672e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT window_start, vehicle_id, avg_speed, event_count\n",
    "FROM delta.`/mnt/gold/iotdata`\n",
    "ORDER BY window_start DESC\n",
    "LIMIT 10;\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5773255393634150,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Final traffic real time",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
